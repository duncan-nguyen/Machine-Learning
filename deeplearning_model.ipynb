{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture \n!pip install transformers datasets torch scikit-learn pandas numpy protobuf==3.20.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T04:10:50.317789Z","iopub.execute_input":"2025-12-30T04:10:50.317984Z","iopub.status.idle":"2025-12-30T04:12:03.413667Z","shell.execute_reply.started":"2025-12-30T04:10:50.317967Z","shell.execute_reply":"2025-12-30T04:12:03.412679Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os, random, re, html, unicodedata\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple, Optional, List\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    roc_auc_score, classification_report, hamming_loss, \n    precision_recall_fscore_support\n)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import classification_report\n\n# -------------------------\n# Cấu hình chung\n# -------------------------\nSEED = 42\nTEXT_COL = \"comment_text\"\nLABEL_COLS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\nSHARED_TOKENIZER = \"distilbert-base-uncased\"\n\nTHR_BIN = 0.8\nKEEP_NORMAL = 100_000\nVAL_RATIO = 0.1\nOUT_DIR = \"./ckpt\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)\n\ntry:\n    _STOPWORDS = set(stopwords.words(\"english\"))\nexcept LookupError:\n    nltk.download(\"stopwords\")\n    _STOPWORDS = set(stopwords.words(\"english\"))\n\n# -------------------------\n# Tiền xử lý & Làm sạch dữ liệu\n# -------------------------\n_COMBINED_RE = re.compile(\n    r\"(https?://\\S+|www\\.\\S+)|(\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b)|(@\\w+)|(<[^>]+>)|([\\x00-\\x1f\\x7f-\\x9f])\",\n    re.IGNORECASE\n)\n_WS_RE = re.compile(r\"\\s+\")\n\ndef fast_clean(s: str) -> str:\n    def _replacer(match):\n        if match.group(1): return \" <URL> \"\n        if match.group(2): return \" <EMAIL> \"\n        if match.group(3): return \" <USER> \"\n        return \" \"\n    s = html.unescape(str(s))\n    s = unicodedata.normalize(\"NFKC\", s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\"))\n    s = _COMBINED_RE.sub(_replacer, s)\n    return _WS_RE.sub(\" \", s).strip()\n\ndef preprocess_text_nltk(s: str, lower=True, keep_punct=True) -> str:\n    s = fast_clean(s)\n    tk = TweetTokenizer(preserve_case=not lower, reduce_len=True, strip_handles=False)\n    tokens = tk.tokenize(s)\n    if not keep_punct:\n        tokens = [t for t in tokens if t in {\"<URL>\", \"<EMAIL>\", \"<USER>\"} or any(c.isalnum() for c in t)]\n    return \" \".join(tokens)\n\ndef apply_text_preprocess_nltk(texts: list, n_jobs=-1, **kwargs):\n    processed = Parallel(n_jobs=n_jobs)(delayed(preprocess_text_nltk)(str(t), **kwargs) for t in texts)\n    return processed\n\ndef load_and_prepare_splits():\n    print(\"--- Đang tải dữ liệu và làm sạch (Drop None) ---\")\n    ds = load_dataset(\"nqdhocai/toxic-comment-detection\")\n    train_df = ds[\"train\"].to_pandas()\n    test_df = ds[\"test\"].to_pandas()\n\n    train_df = train_df.dropna(subset=[TEXT_COL])\n    test_df = test_df.dropna(subset=[TEXT_COL])\n    train_df = train_df[train_df[TEXT_COL].astype(str).str.strip() != \"\"]\n    test_df = test_df[test_df[TEXT_COL].astype(str).str.strip() != \"\"]\n\n    print(\"--- Tiền xử lý văn bản song song ---\")\n    train_df[\"comment_text_proc\"] = apply_text_preprocess_nltk(train_df[TEXT_COL].tolist())\n    test_df[\"comment_text_proc\"] = apply_text_preprocess_nltk(test_df[TEXT_COL].tolist())\n\n    train_df = train_df[train_df[\"comment_text_proc\"].str.strip() != \"\"]\n    test_df = test_df[test_df[\"comment_text_proc\"].str.strip() != \"\"]\n\n    for c in LABEL_COLS:\n        train_df[c] = (pd.to_numeric(train_df[c], errors='coerce').fillna(0) >= THR_BIN).astype(int)\n        test_df[c] = (pd.to_numeric(test_df[c], errors='coerce').fillna(0) >= THR_BIN).astype(int)\n\n    mask_normal = (train_df[LABEL_COLS].sum(axis=1) == 0)\n    train_toxic = train_df[~mask_normal]\n    train_normal = train_df[mask_normal].sample(n=min(KEEP_NORMAL, sum(mask_normal)), random_state=SEED)\n    train_df = pd.concat([train_toxic, train_normal]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_df[\"comment_text_proc\"].values, train_df[LABEL_COLS].values,\n        test_size=VAL_RATIO, random_state=SEED, stratify=train_df[LABEL_COLS].values.sum(axis=1)\n    )\n    return X_train, y_train, X_val, y_val, test_df[\"comment_text_proc\"].values, test_df[LABEL_COLS].values, test_df[TEXT_COL].values\n\nX_train, y_train, X_val, y_val, X_test, y_test, X_test_raw = load_and_prepare_splits()\n\npos = y_train.sum(axis=0)\nneg = len(y_train) - pos\npos_weight = torch.sqrt(torch.tensor(neg / (pos + 1e-6), device=device, dtype=torch.float32))\n\n# -------------------------\n# Định nghĩa Models \n# -------------------------\nclass RNNClassifier(nn.Module):\n    def __init__(self, vocab_size, pad_id, embed_dim=256, hidden=256, num_labels=6, dropout=0.3):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.rnn = nn.RNN(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden * 2, num_labels)\n    def forward(self, input_ids, lengths):\n        x = self.emb(input_ids)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        out, _ = self.rnn(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        x_pool, _ = torch.max(out, dim=1) # Max Pooling\n        return self.fc(self.drop(x_pool))\n\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, pad_id, embed_dim=256, hidden=256, num_labels=6, dropout=0.3):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n        self.drop = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden * 2, num_labels)\n    def forward(self, input_ids, lengths):\n        x = self.emb(input_ids)\n        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        out, _ = self.lstm(packed)\n        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        x_pool, _ = torch.max(out, dim=1) # Max Pooling\n        return self.fc(self.drop(x_pool))\n\nclass EncoderMultiLabel(nn.Module):\n    def __init__(self, model_name, num_labels=6, dropout=0.3, pooling=\"cls\"):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.pooling = pooling\n        h_size = self.encoder.config.hidden_size\n        self.fc = nn.Linear(h_size, num_labels)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n        emb = out[:, 0] if self.pooling == \"cls\" else out.mean(dim=1)\n        return self.fc(self.drop(emb))\n\n# -------------------------\n# Build, Loaders & Utils\n# -------------------------\n@dataclass\nclass ModelSpec:\n    name: str; kind: str; tokenizer_name: str; max_len: int; batch_train: int; \n    batch_eval: int; epochs: int; lr: float; warmup_ratio: float = 0.1; \n    dropout: float = 0.3; embed_dim: int = 256; hidden: int = 256; pooling: str = \"cls\"\n\ndef build_model(spec, tokenizer):\n    if spec.kind == \"encoder\":\n        return EncoderMultiLabel(spec.tokenizer_name, num_labels=len(LABEL_COLS), dropout=spec.dropout, pooling=spec.pooling)\n    else:\n        cls = BiLSTMClassifier if spec.name == \"bilstm\" else RNNClassifier\n        return cls(tokenizer.vocab_size, tokenizer.pad_token_id, spec.embed_dim, spec.hidden, num_labels=len(LABEL_COLS), dropout=spec.dropout)\n\ndef make_loaders(spec, tokenizer):\n    class TextDS(Dataset):\n        def __init__(self, texts, labels): self.texts, self.labels = texts, labels\n        def __len__(self): return len(self.texts)\n        def __getitem__(self, i): return self.texts[i], self.labels[i]\n\n    def collate(batch):\n        texts, labels = zip(*batch)\n        texts = [str(t) for t in texts]\n        enc = tokenizer(texts, padding=True, truncation=True, max_length=spec.max_len, return_tensors=\"pt\")\n        yb = torch.tensor(np.array(labels), dtype=torch.float32)\n        if spec.kind == \"encoder\": return enc[\"input_ids\"], enc[\"attention_mask\"], yb\n        return enc[\"input_ids\"], enc[\"attention_mask\"].sum(dim=1).long(), yb\n\n    return (DataLoader(TextDS(X_train, y_train), batch_size=spec.batch_train, shuffle=True, collate_fn=collate),\n            DataLoader(TextDS(X_val, y_val), batch_size=spec.batch_eval, collate_fn=collate),\n            DataLoader(TextDS(X_test, y_test), batch_size=spec.batch_eval, collate_fn=collate))\n\n# -------------------------\n# Training Loop & Visualization\n# -------------------------\ndef eval_loop(model, loader, spec, criterion):\n    model.eval()\n    total_loss, all_probs, all_trues = 0.0, [], []\n    with torch.no_grad():\n        for batch in loader:\n            inputs = [b.to(device) for b in batch]\n            logits = model(*inputs[:-1])\n            loss = criterion(logits, inputs[-1])\n            total_loss += loss.item() * inputs[-1].size(0)\n            all_probs.append(torch.sigmoid(logits).cpu().numpy())\n            all_trues.append(inputs[-1].cpu().numpy())\n    return total_loss / len(loader.dataset), roc_auc_score(np.vstack(all_trues), np.vstack(all_probs), average=\"macro\"), np.vstack(all_trues), np.vstack(all_probs)\n\ndef find_best_thresholds_per_label(y_true, y_prob, label_cols):\n    \"\"\"Tìm ngưỡng tối ưu cho từng nhãn trên tập Validation\"\"\"\n    grid = np.linspace(0.05, 0.95, 19)\n    thr_map = {}\n    for j, col in enumerate(label_cols):\n        yt, pr = y_true[:, j], y_prob[:, j]\n        best_t, best_f1 = 0.5, -1.0\n        for t in grid:\n            yp = (pr >= t).astype(int)\n            p, r, f1, _ = precision_recall_fscore_support(yt, yp, average='binary', zero_division=0)\n            if f1 > best_f1:\n                best_f1, best_t = f1, t\n        thr_map[col] = best_t\n    return thr_map\n\ndef binarize_with_thresholds(y_prob, thr_map, label_cols):\n    \"\"\"Chuyển xác suất sang 0/1 dựa trên map ngưỡng\"\"\"\n    y_pred = np.zeros_like(y_prob, dtype=int)\n    for i, col in enumerate(label_cols):\n        y_pred[:, i] = (y_prob[:, i] >= thr_map[col]).astype(int)\n    return y_pred\n    \ndef train_one_model(spec: ModelSpec):\n    print(f\"\\n{'='*20} HUẤN LUYỆN: {spec.name.upper()} {'='*20}\")\n    tokenizer = AutoTokenizer.from_pretrained(spec.tokenizer_name)\n    train_loader, val_loader, test_loader = make_loaders(spec, tokenizer)\n    model = build_model(spec, tokenizer).to(device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=spec.lr, weight_decay=0.01 if spec.kind==\"encoder\" else 0)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    scheduler = get_linear_schedule_with_warmup(optimizer, int(len(train_loader)*spec.epochs*spec.warmup_ratio), len(train_loader)*spec.epochs)\n    \n    history = {\"epochs\": [], \"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n    best_auc, best_path = -1.0, f\"{OUT_DIR}/{spec.name}_best.pt\"\n\n    for ep in range(1, spec.epochs + 1):\n        model.train()\n        total_tr_loss = 0.0\n        pbar = tqdm(train_loader, desc=f\"Epoch {ep}\", leave=False)\n        for batch in pbar:\n            optimizer.zero_grad()\n            inputs = [b.to(device) for b in batch]\n            logits = model(*inputs[:-1])\n            loss = criterion(logits, inputs[-1])\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            total_tr_loss += loss.item() * inputs[-1].size(0)\n\n        va_loss, va_auc, _, _ = eval_loop(model, val_loader, spec, criterion)\n        history[\"epochs\"].append(ep); history[\"train_loss\"].append(total_tr_loss/len(train_loader.dataset))\n        history[\"val_loss\"].append(va_loss); history[\"val_auc\"].append(va_auc)\n        print(f\"E{ep} | Tr-Loss: {total_tr_loss/len(train_loader.dataset):.4f} | Val-Loss: {va_loss:.4f} | Val-AUC: {va_auc:.4f}\")\n\n        if va_auc > best_auc:\n            best_auc = va_auc\n            torch.save(model.state_dict(), best_path)\n\n    print(f\"\\n[Final Evaluation] Loading best model from {best_path}...\")\n    model.load_state_dict(torch.load(best_path))\n    \n    _, _, y_val_true, y_val_prob = eval_loop(model, val_loader, spec, criterion)\n    best_thrs = find_best_thresholds_per_label(y_val_true, y_val_prob, LABEL_COLS)\n    \n    te_loss, te_auc, y_test_true, y_test_prob = eval_loop(model, test_loader, spec, criterion)\n    \n    y_test_pred = binarize_with_thresholds(y_test_prob, best_thrs, LABEL_COLS)\n    \n    print(f\"\\n--- TEST REPORT: {spec.name.upper()} ---\")\n    print(f\"Test AUC (Macro): {te_auc:.4f}\")\n    print(\"\\nClassification Report (Tuned Thresholds):\")\n    print(classification_report(y_test_true, y_test_pred, target_names=LABEL_COLS, digits=2))\n            \n    return history\n\ndef plot_learning_curves(all_results):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    colors = {'rnn': 'blue', 'bilstm': 'green', 'distilbert': 'red'}\n    for name, hist in all_results.items():\n        c = colors.get(name)\n        ax1.plot(hist[\"epochs\"], hist[\"train_loss\"], label=f\"{name}-Tr\", ls='-', color=c, alpha=0.3)\n        ax1.plot(hist[\"epochs\"], hist[\"val_loss\"], label=f\"{name}-Val\", ls='--', color=c)\n        ax2.plot(hist[\"epochs\"], hist[\"val_auc\"], label=name, marker='o', color=c)\n    ax1.set_title(\"Loss Curve (Train vs Val)\"); ax1.legend(); ax1.grid(True)\n    ax2.set_title(\"Validation AUC Curve\"); ax2.legend(); ax2.grid(True)\n    plt.show()\n\n# -------------------------\n# Thực thi\n# -------------------------\nMODEL_SPECS = [\n    ModelSpec(\"rnn\", \"rnn_like\", SHARED_TOKENIZER, 256, 128, 256, 5, 1e-3),\n    ModelSpec(\"bilstm\", \"rnn_like\", SHARED_TOKENIZER, 256, 128, 256, 5, 1e-3),\n    ModelSpec(\"distilbert\", \"encoder\", SHARED_TOKENIZER, 128, 16, 32, 3, 1e-5)\n]\n\nfor spec in MODEL_SPECS:\n    final_results = {}\n    final_results[spec.name] = train_one_model(spec)\n\n    plot_learning_curves(final_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T04:14:01.537477Z","iopub.execute_input":"2025-12-30T04:14:01.538224Z"}},"outputs":[{"name":"stdout","text":"--- Đang tải dữ liệu và làm sạch (Drop None) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/919 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc213d8e139d49c3aca1a9d808a47350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002.parquet:   0%|          | 0.00/218M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cdb18c55c914315a989c224ccc8db8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ce7f24c9444fccbf297daa8f8b0b68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46eded183cd04ccba5b199d5ffffd0b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2223065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c555c54a733c48dbaec1300eb77922f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/399 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8155304680be4c41ac3d66c6fcd708a5"}},"metadata":{}},{"name":"stdout","text":"--- Tiền xử lý văn bản song song ---\n\n==================== HUẤN LUYỆN: RNN ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E1 | Tr-Loss: 0.3811 | Val-Loss: 0.2464 | Val-AUC: 0.9512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E2 | Tr-Loss: 0.2332 | Val-Loss: 0.2153 | Val-AUC: 0.9628\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E3 | Tr-Loss: 0.2080 | Val-Loss: 0.2077 | Val-AUC: 0.9662\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E4 | Tr-Loss: 0.1896 | Val-Loss: 0.2049 | Val-AUC: 0.9673\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E5 | Tr-Loss: 0.1760 | Val-Loss: 0.2034 | Val-AUC: 0.9679\n\n==================== HUẤN LUYỆN: BILSTM ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"E1 | Tr-Loss: 0.3567 | Val-Loss: 0.2243 | Val-AUC: 0.9591\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/1109 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7ac1b8256a4de2987699a25f8becfe"}},"metadata":{}}],"execution_count":null}]}